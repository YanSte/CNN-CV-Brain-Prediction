{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yannicksteph/cnn-cv-brain-prediction?scriptVersionId=143948558\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# | CNN | CV | Brain | Prediction |\n## Convolutional Neural Networks (CNN) with Computer Vision (CV) for Brain Prediction\n# <b>1 <span style='color:#78D118'>|</span> Introduction</b>\n\nGlioblastoma, the most common and aggressive form of brain cancer in adults, poses significant challenges in diagnosis and treatment. The presence of MGMT promoter methylation in the tumor has been identified as a crucial prognostic factor and an indicator of chemotherapy responsiveness. However, the current genetic analysis of brain cancer requires invasive procedures and time-consuming processes.\n\nThe objective of this project is to improve the diagnosis and treatment strategies for glioblastoma patients, minimizing the need for invasive procedures and streamlining the genetic analysis process. By leveraging radiogenomics, the aim is to develop a non-invasive method to predict the genetic profile of the tumor solely through imaging.\n\nTo address these issues, the Radiological Society of North America (RSNA) and the Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) have collaborated on a competition focusing on glioblastoma diagnosis and treatment planning. The competition involves using MRI scans to develop a model that can accurately predict the genetic subtype of glioblastoma by detecting the presence of MGMT promoter methylation.\n\nSuccessful outcomes from this competition will contribute to less invasive diagnostic procedures and more tailored treatment approaches for brain cancer patients. This abstract provides an overview of the project's objectives, the competition's context, and the potential impact on the management and survival rates of individuals affected by glioblastoma.\n\n## Dataset Overview\n\nThe Radiological Society of North America (RSNAÂ®) is a non-profit organization representing 31 radiologic subspecialties from 145 countries worldwide. RSNA promotes excellence in patient care and healthcare delivery through education, research, and technological innovation.\n\nRSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world's largest radiology conference, and is dedicated to shaping the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its establishment. Additionally, RSNA actively supports and facilitates research in medical imaging artificial intelligence (AI) by sponsoring ongoing AI challenge competitions.\n\nThe Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) is committed to advancing research, education, and practice in the field of medical image computing, computer-assisted interventions, biomedical imaging, and medical robotics. The society achieves this objective by organizing high-quality international conferences, workshops, tutorials, and publications that promote the exchange and dissemination of advanced knowledge, expertise, and experiences produced by leading institutions, scientists, physicians, and educators worldwide.\n\nA complete list of acknowledgments can be found on this page.\n\n[RSNA-MICCAI Brain Tumor Radiogenomic Classification](https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification/data?select=train_labels.csv)\n\n## Research Efforts\nDuring the course of this project, we conducted extensive research to explore various methodologies for predicting the genetic subtype of glioblastoma based on MGMT promoter methylation. One of the methods we investigated was the use of the Unit-net architecture, a convolutional neural network designed specifically for medical image analysis.\n\n[| UNIT-NET | CV | BRAIN | Classification |](https://www.kaggle.com/code/yannicksteph/rsna-miccai-brain-tumor-classification)\n\nHowever, despite our efforts, we did not achieve conclusive results with the Unit-net model. The complexity and variability of glioblastoma tumors, as well as the limited availability of labeled data, presented significant challenges in training an effective Unit-net model for this task. As a result, we decided to pursue alternative approaches that showed more promise in accurately predicting the genetic subtype of glioblastoma.\n\n## Objectives\n- Predict the genetic subtype of glioblastoma by detecting the presence of MGMT promoter methylation value between 0 to 1.\n\n## References and Research Sources\n- MGMT\n    - [Is it Possible to Predict MGMT Promoter Methylation from Brain Tumor MRI Scans using Deep Learning Models](https://arxiv.org/abs/2201.06086)\n    - [Automatic Prediction of MGMT Status in Glioblastoma via Deep Learning-Based MR Image Analysis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7530505/)\n    - [MRI-Based Deep-Learning Method for Determining Glioma MGMT Promoter Methylation Status](https://www.ajnr.org/content/42/5/845.abstract)\n    - [Improving MGMT methylation status prediction of glioblastoma through optimizing radiomics features using genetic algorithm-based machine learning approach](https://www.nature.com/articles/s41598-022-17707-w)\n- Data augmentation\n    - [Data augmentation for deep learning based accelerated MRI reconstruction with limited data](https://arxiv.org/abs/2106.14947)\n    - [Data augmentation: how to overcome small radiology datasets](https://www.quantib.com/blog/image-augmentation-how-to-overcome-small-radiology-datasets?hs_amp=true)\n\n\n\n## Implementation\nTo achieve the aforementioned objectives, we will follow these steps:\n\n- **Training Setup Explained**\n- **Setup**\n- **Data Retrieval**\n- **Data Preparation** \n- **Model Creation** \n- **Model Training** \n- **Model Evaluation** \n\n# <b>2 <span style='color:#78D118'>|</span> Training Setup Explained</b>\n\n## Cross-Validation\n\n- **Splitting Method**\n    - **Type:** Stratified K-Fold Cross-Validation\n        - **Explanation:** Employed to ensure a balanced class distribution within each fold, enhancing model assessment and generalization.\n        - **Configuration:**\n            - **Number of Folds:** Typically set to 5.\n            - **Validation Fold:** Specifically, the first fold is designated as the validation fold in this case.\n\n## Processing\n\n- **Width, Height, Channels:**\n    - Images are initially resized to a uniform dimension of 128x128 pixels. Furthermore, IRM converted to image give a grayscale format, resulting in a single channel for each image. \n    - It's important to note that due to Kaggle limitations, we cannot further increase the image size to 224x224 pixels.\n- **Sequence:**\n    - For each input we give a sequence of 32 sequential images, which are processed in batches. \n    - This sequence captures the temporal dimension of the MRI scans and allows the model to analyze a series of images to make predictions.\n- **Scale:**\n    - During preprocessing, images are scaled down to 85% of their original size. \n    - This scaling operation is applied at to both the test and validation datasets. \n    - For the raining dataset we apply augmentation which we will see later.\n    - The purpose of this scaling is to remove any empty space around the brain border, ensuring that the model focuses primarily on the brain region itself.\n- **Central Focus:**\n    - In our dataset processing pipeline, we place a strong emphasis on central regions where is the ROI.\n    - Specifically, we prioritize the center image and include 16 images before and 16 images after the central image in each sequence. \n    - This approach ensures that the most informative parts of the MRI scans, corresponding to the central brain area and the ROI, receive the most attention during model training.\n\n## Augmentation\n\n- **Data Augmentation**\n    - We expand our original dataset by a significant 400%, effectively quadrupling the available training data. This is essential as the original dataset consists of only approximately 500 samples, with the validation set containing just 100 samples.\n- **Crop Augmentation**\n    - Images are randomly cropped while preserving between 85% and 95% of their original size. This enhances the model's ability to recognize different brain regions.\n- **Rotation Augmentation**\n    - Random rotations ranging from 4 to 12 degrees aid the model in becoming orientation-invariant, allowing it to handle variations in the orientation of brain scans effectively.\n- **Translation Augmentation**\n    - Random translations are applied both horizontally (2 to 6 pixels) and vertically (0 to 2 pixels), simulating minor positional variations commonly encountered in medical imaging.\n- **Blur Augmentation**\n    - A random blur effect is introduced with a 10% to 15% probability, mimicking real-world imaging imperfections and improving the model's generalization.\n- **Contrast and Brightness Augmentation**\n    - Image contrast is dynamically scaled between 0.8 and 1.2, while brightness is adjusted between -2 and 2. This adaptation accommodates varying lighting conditions, making the model more robust to different lighting scenarios.\n\n## Train\n\n- **Batch Size**\n    - Training is performed in batches of 8 images at a time, a limitation imposed by Kaggle.\n- **Epochs**\n    - The model undergoes 32 training epochs.\n- **Optimizer**\n    - **Type:** Stochastic Gradient Descent (SGD)\n        - **Explanation:**  Stochastic Gradient Descent (SGD) iteratively adjusts model weights using gradients computed from training data to update to minimize the loss.\n        - **Configuration:**\n            - **Learning Rate:** A learning rate of 0.001 strikes a balance between convergence speed and stability.\n- **Loss Function**\n    - **Type:** Binary Cross-Entropy\n        - **Explanation:**  Binary Cross-Entropy is used for training, specifically suited for binary classification tasks such as this one.\n- **Compilation Metrics**\n    - **Type:** Area Under the ROC Curve (AUC)\n        - **Explanation:** AUC is employed as a metric, measuring the model's ability to discriminate between positive and negative classes.\n\n### Model Architecture\n\n- **DeepScanModel**\n    - The Model is a 3D Convolutional Neural Network (CNN) designed specifically for processing sequences of medical images. \n    - It takes four channels corresponding to four image sequences, concatenates them, and processes them to perform binary classification.\n","metadata":{}},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Setup</b>\n\n## <b>2.1 <span style='color:#78D118'>|</span> Imports</b>","metadata":{}},{"cell_type":"code","source":"!pip install -q pydicom\n!pip install -q git+https://github.com/YanSteph/SKit.git","metadata":{"id":"oo5zZT0Z8dNL","outputId":"6bfa9cee-3e7a-4898-95ab-5471e105963b","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:34:21.403788Z","iopub.execute_input":"2023-09-13T18:34:21.404161Z","iopub.status.idle":"2023-09-13T18:35:10.685893Z","shell.execute_reply.started":"2023-09-13T18:34:21.404132Z","shell.execute_reply":"2023-09-13T18:35:10.684599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport numpy as np\nimport random\n\n# Dicom\nimport pydicom\n\n# Enum\nfrom enum import Enum\n\n# CV\nimport cv2\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import (\n    Model,\n    load_model\n)\nfrom tensorflow.keras.callbacks import (\n    Callback, \n    ModelCheckpoint, \n    EarlyStopping\n)\nfrom tensorflow.keras.layers import (\n    Input,\n    Conv3D,\n    BatchNormalization,\n    MaxPooling3D,\n    MaxPool3D,\n    Flatten,\n    Dense,\n    Dropout,\n    Resizing,\n    Rescaling,\n    RandomFlip,\n    RandomRotation,\n    concatenate,\n    GlobalAveragePooling3D,\n    Reshape,\n    LeakyReLU,\n    ReLU\n)\n\n# Keras\nimport keras\nfrom keras.utils.vis_utils import plot_model\n\n\n# Skit\nfrom skit.Debug import Debug\nfrom skit.InternalDebug import InternalDebug\nfrom skit.image import average_image_size\nfrom skit.dataset import stratifiedTrainValidSplit\nfrom skit.Summarizable import Summarizable\nfrom skit.tensorflow import configure_gpu_memory\nfrom skit.dicom import (\n    DICOMLoader, \n    ImageFormat\n)\nfrom skit.utils import (\n    ls, \n    mkdir, \n    count_files\n)\nfrom skit.show import (\n    show_text, \n    show_images, \n    show_donut, \n    show_history, \n    show_best_history,\n    show_confusion_matrix, \n    show_donut,\n    show_histogram\n)","metadata":{"id":"v8J2Qtod76GK","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:10.688494Z","iopub.execute_input":"2023-09-13T18:35:10.688838Z","iopub.status.idle":"2023-09-13T18:35:11.285562Z","shell.execute_reply.started":"2023-09-13T18:35:10.688809Z","shell.execute_reply":"2023-09-13T18:35:11.284429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>2.2 <span style='color:#78D118'>|</span> Constants</b>","metadata":{}},{"cell_type":"code","source":"class MRIType(Enum):\n    FLAIR = \"FLAIR\"\n    T1w = \"T1w\"\n    T1wCE = \"T1wCE\"\n    T2w = \"T2w\"\n    \nclass DatasetType(Enum):\n    TRAIN = \"train\"\n    VALIDATION = \"validation\"\n    TEST = \"test\"","metadata":{"id":"7khZRVY576GL","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:11.286934Z","iopub.execute_input":"2023-09-13T18:35:11.288039Z","iopub.status.idle":"2023-09-13T18:35:11.293824Z","shell.execute_reply.started":"2023-09-13T18:35:11.287993Z","shell.execute_reply":"2023-09-13T18:35:11.292823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Global\n# ----\nVERSION         = \"V1\"\nVERBOSITY       = 2\nSEED            = 123\nSCAN_CATEGORIES = [mri_type.value for mri_type in MRIType]\nEXCLUDED_IDS    = [109, 123, 709]\n\n# Paths\n# ----\nRUN_DIR = './run'\nINPUT_PATH = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification\"\n\n# Train\nTRAIN_DATASET_PATH = INPUT_PATH + \"/train\"\nTRAIN_DATASET_DF_DIR = INPUT_PATH + \"/train_labels.csv\"\n\n# Test\nTEST_DATASET_PATH = INPUT_PATH + \"/test\"\nTEST_DATASET_DF_DIR = INPUT_PATH + \"/sample_submission.csv\"\n\n# Submission\nSUBMISSION_DATASET_DF_DIR = '/kaggle/working/submission.csv'\n\n# TF Callback Paths\n# ----\nLOGS_PATH = f'{RUN_DIR}/logs'\nBEST_MODEL_PATH = f'{RUN_DIR}/models'\nBEST_MODEL_H5_DIR = f'{BEST_MODEL_PATH}/model_{VERSION}.h5'\n\n# Fold\n# ----\nNUM_SPLIT_FOLDS = 5\nSELECTED_VALIDATION_FOLD = 1\n\n# Dicom Loader\n# ----\nMAX_THREADS_DICOM_LOADER = 8\n\n# Image\n# ----\nIMG_WIDTH_SIZE, IMG_HEIGHT_SIZE, IMG_CHAN = (128, 128, 1)\nIMG_SIZE = (IMG_WIDTH_SIZE, IMG_HEIGHT_SIZE)\n\nIMG_SEQ                  = 32\nIMG_SCALE                = .95\nIMG_ROTATE               = 0 \nIMG_ENABLE_CENTRAL_FOCUS = True\n\nSHUFFLE    = True\n\n# Augmentation\n# ----\nAUGMENTATION_FRACTION               = 4\nAUGMENTATION_CROP_LIMITS            = (0.85, 0.95)\nAUGMENTATION_ROTATION_LIMITS        = (4, 12)\nAUGMENTATION_TRANSLATION_X_Y_LIMITS = ((2, 6), (0, 2))\nAUGMENTATION_BLUR                   = (0, 0.15)\nAUGMENTATION_CONSTRAST_BRIGHT       = ((0.8, 1.2),(-2, 2))\n\n# Model\n# ----\nINPUT_SHAPE = (IMG_WIDTH_SIZE, IMG_HEIGHT_SIZE, IMG_SEQ, IMG_CHAN) # Format sample: (128, 128, 64, 1)\n\nMODEL_NAME = \"Mult3DCNN4Input\"\nBATCH_SIZE = 8\nEPOCHS     = 26\n\nCOMPILE_OPTIMIZER = SGD(learning_rate =0.001)\nCOMPILE_LOSS = 'binary_crossentropy'\nCOMPILE_METRICS = [AUC(name='auc')]\n\n# TF Callback\n# ----\nTF_CALL_BACK_BEST_MODEL_MONITOR  = \"val_auc\"\nTF_CALL_BACK_EARLY_STOP_MONITOR  = \"auc\"\nTF_CALL_BACK_EARLY_STOP_PATIENTE = 6","metadata":{"id":"PXWk1s0G76GL","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:02:42.999987Z","iopub.execute_input":"2023-09-13T19:02:43.000385Z","iopub.status.idle":"2023-09-13T19:02:43.026276Z","shell.execute_reply.started":"2023-09-13T19:02:43.000331Z","shell.execute_reply":"2023-09-13T19:02:43.025219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SEED is not None:\n    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.276557Z","iopub.execute_input":"2023-09-13T18:35:14.277168Z","iopub.status.idle":"2023-09-13T18:35:14.282845Z","shell.execute_reply.started":"2023-09-13T18:35:14.277134Z","shell.execute_reply":"2023-09-13T18:35:14.28176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#78D118'>|</span> Debug</b>","metadata":{}},{"cell_type":"code","source":"Debug.set_debug_mode(False)\n\n# Train\nDEBUG_DICOM_TRAIN_AUGMENTATION       = False\nDEBUG_SCANDATASET_TRAIN_AUGMENTATION = False\n\n# Validation\nDEBUG_DICOM_VALIDATION       = False\nDEBUG_SCANDATASET_VALIDATION = False\n\n# Test\nDEBUG_DICOM_TEST       = False\nDEBUG_SCANDATASET_TEST = False","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.284478Z","iopub.execute_input":"2023-09-13T18:35:14.284858Z","iopub.status.idle":"2023-09-13T18:35:14.29434Z","shell.execute_reply.started":"2023-09-13T18:35:14.2848Z","shell.execute_reply":"2023-09-13T18:35:14.293343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>2.3 <span style='color:#78D118'>|</span> Methods</b>","metadata":{}},{"cell_type":"code","source":"class DICOMLoaderAugmentation(DICOMLoader):\n    def __init__(self,\n        df,\n        input_path,\n        scan_categories,\n        fraction_augmented                = 0,\n        crop_limits                       = None,\n        rotation_limits                   = None, \n        translation_x_y_limits            = None, \n        blur_limits                       = None,\n        contrast_bright_alpha_beta_limits = None,\n        num_imgs                          = None,\n        size                              = (224, 224),\n        rotate_angle                      = 0,\n        enable_center_focus               = False,\n        id_column_name                    = \"ID\",\n        label_column_name                 = \"Label\",\n        image_format                      = ImageFormat.WHDC,\n        max_threads                       = 8,\n        image_file_sorter                 = lambda x: int(x[:-4].split(\"-\")[-1]),\n        shuffle                           = False,\n        seed                              = None,\n        debug_mode                        = False\n    ):        \n        self.__crop_limits                       = crop_limits\n        self.__rotation_limits                   = rotation_limits\n        self.__translation_x_y_limits            = translation_x_y_limits\n        self.__blur_limits                       = blur_limits\n        self.__contrast_bright_alpha_beta_limits = contrast_bright_alpha_beta_limits\n        self.__seed                              = seed\n        self.__debug                             = InternalDebug(debug_mode=debug_mode)\n        self.__fraction_augmented                = fraction_augmented\n        df = df.copy()\n        df = DICOMLoaderAugmentation.__data_augmentation(df, fraction_augmented, shuffle, self.__debug, seed)\n          \n        super().__init__(\n            df, \n            input_path,\n            scan_categories, \n            num_imgs, \n            size, \n            0, # NOTE: We define her the scale for augmented data\n            rotate_angle,\n            enable_center_focus,\n            id_column_name, \n            label_column_name, \n            image_format,\n            max_threads,\n            image_file_sorter,\n            False\n        )\n        \n    # ---------------- #\n    # Enum\n    # ---------------- #\n    \n    class OrigineType(Enum):\n        ORIGINAL = \"ORIGINAL\"\n        AUGMENTED = \"AUGMENTED\"\n\n    # ---------------- #\n    # Public\n    # ---------------- #\n\n    def load_all_scans(\n        self,\n        row,\n        show_progress=True\n    ):\n        self.__debug.log(\"== load_all_scans ==\")\n        \n        # Load all scans for the current index\n        # ----\n        scans_images = super().load_all_scans(row, show_progress)\n        \n        if self.__fraction_augmented > 0 and self.__is_augmented(row):\n            scans_images = self.__augmented(row, scans_images)\n       \n        return scans_images\n    \n    # ---------------- #\n    # Private\n    # ---------------- #\n    \n    # ---- ---- ---- #\n    # Augmentation\n    # ---- ---- ---- #\n    \n    def __augmented(self, row, scans_images):\n        self.__debug.log(\"== augmented ==\")\n        \n        # Radom Crop\n        # ----\n        if self.__crop_limits is not None:\n            min_crop, max_crop = self.__crop_limits\n        \n            crop_random = random.uniform(min_crop, max_crop)\n        \n        # Radom Rotation\n        # ----\n        if self.__rotation_limits is not None:\n            min_rot, max_rot = self.__rotation_limits\n        \n            rotation_random = random.uniform(min_rot, max_rot) * random.choice([-1, 1])\n        \n        # Radom Translation \n        # ----\n        if self.__translation_x_y_limits is not None:\n            tx, ty = self.__translation_x_y_limits\n        \n            min_tx, max_tx = tx\n            min_ty, max_ty = ty\n        \n            tx_random = random.uniform(min_tx, max_tx) * random.choice([-1, 1])\n            ty_random = random.uniform(min_ty, max_ty) * random.choice([-1, 1])\n        \n        # Radom Blur \n        # ----\n        if self.__blur_limits is not None:\n            min_blur, max_blur = self.__blur_limits\n        \n            blur_random = random.uniform(min_blur, max_blur)\n\n        # Radom Contrast and Brightness\n        # ----\n        if self.__contrast_bright_alpha_beta_limits is not None:\n            alpha, meta = self.__contrast_bright_alpha_beta_limits\n        \n            min_alpha, max_alpha = alpha\n            min_beta, max_beta   = meta\n        \n            alpha_random = random.uniform(min_alpha, max_alpha)\n            beta_radom = random.uniform(min_beta, max_beta)\n        \n        # Log\n        # ----\n        self.__debug.log(\n                f\"Generation\\n\"\n                f\"- Crop: {crop_random}\\n\"\n                f\"- Rotation: {rotation_random}\\n\"\n                f\"- translation x: {tx_random}\\n\"\n                f\"- translation y: {ty_random}\\n\"\n                f\"- Blur: {blur_random}\\n\"\n                f\"- Contrast Brightness alpha: {alpha_random}\\n\"\n                f\"- Contrast Brightness Beta: {beta_radom}\\n\"               \n        )\n\n        for scan_type, images in scans_images.items():\n            \n            augmented_images = []\n            \n            # Format Normalize\n            # ----\n            images = self.format(images, \"normalize\")\n      \n            for image in images:\n                # Remove chan\n                # ----\n                image = np.squeeze(image, axis=-1)\n                \n                # Crop\n                # ----\n                if self.__crop_limits is not None:\n                    image = self.__crop_img(image, crop_random)\n                \n                # Resize\n                # ----\n                image = self._resize_img(image)\n                \n                # Rotation\n                # ----\n                if self.__rotation_limits is not None:\n                    image = self.__rotation_img(image, rotation_random)\n                \n                # Translation\n                # ----\n                if self.__translation_x_y_limits is not None:\n                    image = self.__translation_img(image, tx_random, ty_random)\n                \n                # Blur\n                # ----\n                if self.__blur_limits is not None:\n                    image = self.__blur_img(image, 3, blur_random)\n                \n                # Constrast and Brightness\n                # ----\n                if self.__contrast_bright_alpha_beta_limits is not None:\n                    image = self.__contrast_and_brightness_img(image, alpha_random, beta_radom)\n                  \n                # Save\n                # ----\n                augmented_images.append(image)\n        \n            # Numpy array\n            # ----\n            images = np.array(augmented_images)\n            \n            # Add chan\n            # ----\n            images = np.expand_dims(images, axis=-1)\n            \n            # Format Train\n            # ----\n            images = self.format(images, \"default\")\n\n            scans_images[scan_type] = images\n        \n        return scans_images\n\n    def __blur_img(self, image, kernel_size, blur_random):\n        return cv2.GaussianBlur(image, (kernel_size, kernel_size), blur_random)\n\n    def __translation_img(self, image, tx, ty):\n        height, width = image.shape[:2]\n        translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n        return cv2.warpAffine(image, translation_matrix, (width, height))\n    \n    def __rotation_img(self, image, rotation):\n        height, width = image.shape[:2]\n        center = (width / 2, height / 2)\n        rotation_matrix = cv2.getRotationMatrix2D(center, rotation, 1.0)\n        return cv2.warpAffine(image, rotation_matrix, (width, height))\n    \n    def __contrast_and_brightness_img(self,image, alpha, beta):\n        return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n    \n    def __crop_img(self, image, crop):\n        # Skip if no crop\n        # ----\n        if crop <= 0:\n            return image\n\n        # Calculate the center of the image\n        # ----\n        center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n\n        # Calculate the dimensions of the scaled image\n        # ----\n        width_scaled, height_scaled = image.shape[1] * crop, image.shape[0] * crop\n\n        # Calculate the coordinates for cropping the image\n        # ----\n        left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n        top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n\n        # Crop the image using the calculated coordinates\n        # ----\n        return image[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n    \n    def __is_augmented(self, row):\n        return self.df.loc[row, \"origine_type\"] == DICOMLoaderAugmentation.OrigineType.AUGMENTED.value\n    \n    # ---------------- #\n    # Private Static\n    # ---------------- #\n   \n    @staticmethod\n    def __data_augmentation(df, percentage, shuffle, debug, seed=None):\n        if percentage <= 0:\n            return df\n        debug.log(\"== __data_augmentation ==\")\n        \n        df[\"origine_type\"] = DICOMLoaderAugmentation.OrigineType.ORIGINAL.value\n        \n        # Calculate the number of times to copy\n        # ----\n        num_copies = int(percentage)\n        debug.log(\"num copies:\", num_copies)\n        rest_num_copies = DICOMLoaderAugmentation.__extract_first_decimal(percentage)\n        debug.log(\"rest copies:\", rest_num_copies, \"%\")\n        \n        # Calculate the number of rows to copy\n        # ----\n        num_rows = len(df)\n        debug.log(\"Initial size:\", num_rows)\n        \n        # Initialize a new DataFrame with copy the structure\n        # ----\n        augmented_df = df.copy().iloc[0:0]\n        \n        # Copy the DataFrame 'num_copies' times (525% = 500%)\n        # ----\n        for _ in range(num_copies):\n            random_sample = df.sample(n=num_rows, random_state=seed)\n            augmented_df = pd.concat([augmented_df, random_sample], ignore_index=True)\n        \n        # Copy the remaining percentage (525% = 25%)\n        # ----\n        if rest_num_copies > 0:\n            nombre_d_echantillons = int(num_rows * (rest_num_copies / 100))\n            echantillons_aleatoires = df.sample(n=nombre_d_echantillons, random_state=seed)\n            augmented_df = pd.concat([augmented_df, echantillons_aleatoires], ignore_index=True)\n               \n        # Identified as Augmented\n        # ----\n        augmented_df[\"origine_type\"] = DICOMLoaderAugmentation.OrigineType.AUGMENTED.value\n        debug.log(\"Augmented size add:\", len(augmented_df))\n        \n        # Merge with Original\n        # ----\n        df = pd.concat([df, augmented_df], ignore_index=True)\n        \n        # Suffle\n        # ----\n        if shuffle:\n            df = df.sample(frac=1, random_state=seed)\n          \n        # Reset Ids\n        # ----\n        df = df.reset_index(drop=True)\n        debug.log(\"Augmented size Total:\", len(df))\n        \n        return df\n    \n    @staticmethod\n    def __extract_first_decimal(number):\n        # Convert the number to a string for easier manipulation\n        number_str = str(number)\n\n        # Find the index of the decimal point\n        decimal_point_index = number_str.find(\".\")\n\n        # Extract the first decimal digit from the string\n        if decimal_point_index != -1 and decimal_point_index + 1 < len(number_str):\n            first_decimal_digit = number_str[decimal_point_index + 1]\n            \n            if len(first_decimal_digit) == 1:\n                first_decimal_digit += \"0\"\n        else:\n            # If there's no decimal part, return 0\n            first_decimal_digit = \"0\"\n\n        return int(first_decimal_digit)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T18:35:14.296248Z","iopub.execute_input":"2023-09-13T18:35:14.296636Z","iopub.status.idle":"2023-09-13T18:35:14.339232Z","shell.execute_reply.started":"2023-09-13T18:35:14.296601Z","shell.execute_reply":"2023-09-13T18:35:14.338413Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScanDataset(Sequence, Summarizable):\n    def __init__(\n        self,\n        dicom_loader,\n        batch_size,\n        subset     = \"train\",\n        shuffle    = True,\n        debug_mode = False\n    ):\n        \"\"\"\n        Initializes the ScanDataset object.\n\n        Parameters\n        ----------\n        dicom_loader : object\n            The DICOMLoader object to load DICOM images.\n        batch_size : int\n            The size of each batch.\n        subset: Subset of the data to return.\n          One of \"training\", \"validation\" or \"other\".\n          training and validation give the y_bath\n        shuffle : bool, optional\n            Whether to shuffle the dataset.\n        debug_mode : bool, optional\n            Whether to print debug information.\n        \"\"\"\n        self.__dicom_loader = dicom_loader\n        self.__batch_size   = batch_size\n        self.__is_trainable = subset.lower() in [\"validation\", \"train\"]\n        self.__shuffle      = shuffle\n        self.__debug        = InternalDebug(debug_mode=debug_mode)\n        self.__indices      = np.arange(self.__dicom_loader.len)\n        \n        if self.__shuffle:\n            np.random.shuffle(self.__indices)\n        \n    # ---------------- #\n    # Public methods\n    # ---------------- #\n\n    def show_batch(\n        self,\n        row,\n        columns=None,\n        figure_size=(5, 5),\n        color_map='hot'\n    ):\n        # Get the batch of images and labels (if in training mode)\n        # ----\n        if self.__is_trainable:\n            x_batch, y_batch = self[row]\n        else:\n            x_batch = self[row]\n            y_batch = None\n\n        # Determine the number of columns to show\n        # ----\n        if columns == None:\n            columns = self.__dicom_loader.num_imgs\n            \n        # Determine the number of input tensors\n        # ----\n        num_input_tensors = len(self.__dicom_loader.scan_categories)\n\n        # Loop through each sub-batch in the main batch\n        # ----\n        for i in range(len(x_batch[0])):\n            # Display the batch number and label (if available)\n            # ----\n            label_info = f\" Label: {y_batch[i]}\" if y_batch is not None else \"\"\n\n            # Loop through each input tensor\n            # ----\n            for j in range(num_input_tensors):\n                images = x_batch[j][i]\n                scan_type = self.__dicom_loader.scan_categories[j]\n\n                if self.__dicom_loader.image_format == ImageFormat.WHDC:\n                    images = ImageFormat.swap_dimensions(images, ImageFormat.DWHC)\n\n                # Generate labels for each image in the set\n                # ----\n                labels = [\n                            f\"Batch: {i + 1} \\nImg: {k + 1} \\nType: {scan_type} \\nLabel:{label_info}\"\n                            for k in range(len(images))\n                         ]\n\n                # Show\n                # ----\n                show_images(\n                    images,\n                    y=labels,\n                    columns=columns,\n                    figure_size=figure_size,\n                    color_map=color_map\n                )\n\n    # Overriding the summary method\n    def summary(self, train_dataset=None):\n        super().summary()\n\n        print(\"Additional summary details specific:\")\n\n        # 0 is the index of the first batch\n        # ----\n        if self.__is_trainable:\n            batch_x, batch_y = self[0]\n        else:\n            batch_x = self[0]\n\n        # Checking the batch format\n        # ----\n        print(\"Batch_x format:\")\n        for i, x in enumerate(batch_x):\n          print(f\"- Scan type {i+1}: {x.shape}\")\n\n        if self.__is_trainable:\n            print(f\"Batch_y format: {batch_y.shape}\")\n            \n        print(\"=\" * 40)\n\n    def on_epoch_end(self):\n        \"\"\"\n        Shuffles the dataset at the end of each epoch if shuffle is True.\n        \"\"\"\n        if self.__shuffle:\n            np.random.shuffle(self.__indices)\n\n    # ---------------- #\n    # Private methods\n    # ---------------- #\n\n    def __getitem__(self, ids):\n        self.__debug.log(\"== __getitem__ ==\")\n        \"\"\"\n        Retrieves a batch of data by batch index.\n\n        Parameters\n        ----------\n        ids : int\n            The batch index.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the batch of images and labels.\n        \"\"\"\n        # Calculate the start and end indices for the batch\n        # ----\n        from_id = ids * self.__batch_size\n        to_id = (ids + 1) * self.__batch_size\n\n        self.__debug.log(f\"Batch ID: {ids}\")\n         \n        # Get the indices and labels for the current batch\n        # ----\n        batch_indices = self.__indices[from_id: to_id]\n   \n        self.__debug.log(\"batch_indices:\", batch_indices.tolist())\n\n        batches_y = []\n\n        # Initialize a list to hold batches for each input tensor\n        # ----\n        batches_x = [[] for _ in range(len(self.__dicom_loader.scan_categories))]\n\n        # Loop through each index in the batch\n        # ----\n        for i in batch_indices:\n            self.__debug.log(\"Processing batch index:\", i)\n\n            # Store label\n            # ----\n            label = self.__dicom_loader.gel_label(i)\n            batches_y.append(label)\n            self.__debug.log(\"Label:\", label)\n\n            # Load all scans for the current index\n            # ----\n            batch_x_image_paths = self.__dicom_loader.load_all_scans(i, show_progress=False)\n\n            # Loop through each scan type and its corresponding images\n            # ----\n            for j, (scan_type, images) in enumerate(batch_x_image_paths.items()):\n                self.__debug.log(\"Processing Scan Type:\", scan_type, \"Number of Images Loaded:\", len(images))\n                batches_x[j].append(images)\n\n        # Convert to batch x y\n        # ----\n        batch_x = [np.array(b) for b in batches_x]\n        batch_y = np.array(batches_y)\n\n        self.__debug.log(f\"Final batch shapes - batch_x: {[x.shape for x in batch_x]}, batch_y: {batch_y}\")\n\n        # Return the image batches and labels if in training mode, otherwise just the image batches\n        # ----\n        if self.__is_trainable:\n            return batch_x, batch_y\n        else:\n            return batch_x\n\n    def __len__(self):\n        \"\"\"\n        Calculates the number of batches in the dataset.\n\n        Returns\n        -------\n        int\n            The number of batches.\n        \"\"\"\n        return int(np.ceil(self.__dicom_loader.len / self.__batch_size))","metadata":{"id":"3EV7Q3Jk76GM","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.340837Z","iopub.execute_input":"2023-09-13T18:35:14.341206Z","iopub.status.idle":"2023-09-13T18:35:14.366398Z","shell.execute_reply.started":"2023-09-13T18:35:14.341148Z","shell.execute_reply":"2023-09-13T18:35:14.365423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>3. <span style='color:#78D118'>|</span> Data Retrieval</b>\n\n","metadata":{}},{"cell_type":"code","source":"# Train\ntrain_df = pd.read_csv(TRAIN_DATASET_DF_DIR)\ntrain_df.rename(columns = { \"BraTS21ID\": \"ID\", \"MGMT_value\": \"Label\"}, inplace=True)\n\nindex_to_remove = train_df[train_df['ID'].isin(EXCLUDED_IDS)].index\ntrain_df.drop(index_to_remove, inplace=True)\n\ntrain_df.reset_index(drop=True, inplace=True)\nshow_text(\"b\", \"Train\", False)\ndisplay(train_df.head())\n\n# Test\ntest_df = pd.read_csv(TEST_DATASET_DF_DIR)\ntest_df.rename(columns = { \"BraTS21ID\": \"ID\", \"MGMT_value\": \"Label\"}, inplace=True)\nshow_text(\"b\", \"Test\", False)\ndisplay(test_df.head())","metadata":{"id":"GA2m949u76GL","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.367747Z","iopub.execute_input":"2023-09-13T18:35:14.368287Z","iopub.status.idle":"2023-09-13T18:35:14.427652Z","shell.execute_reply.started":"2023-09-13T18:35:14.368251Z","shell.execute_reply":"2023-09-13T18:35:14.426788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>4 <span style='color:#78D118'>|</span> Data Preparation</b>","metadata":{}},{"cell_type":"markdown","source":"### <b>4.1 <span style='color:#78D118'>|</span> Split Data</b>","metadata":{}},{"cell_type":"code","source":"# Split Train, valid using Stratified K-Folds\n# ----\ntrain_df, valid_df = stratifiedTrainValidSplit(\n    train_df,\n    x_feature_columns = ['ID'],\n    y_target_columns = ['Label'],\n    num_splits = NUM_SPLIT_FOLDS,\n    selected_fold = SELECTED_VALIDATION_FOLD,\n    seed = SEED,\n    shuffle = SHUFFLE\n)\n\n# Show\n# ----\nshow_donut(\n    [len(train_df), len(valid_df)],\n    [\"Train\", \"Validation\"], # TODO: Add test\n    colors = [\"lightsteelblue\",\"coral\"],\n    figsize = (8,8),\n    title = \"Dataset Distribution\"\n)","metadata":{"id":"4q3Ii1hY76GM","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.431456Z","iopub.execute_input":"2023-09-13T18:35:14.43231Z","iopub.status.idle":"2023-09-13T18:35:14.733355Z","shell.execute_reply.started":"2023-09-13T18:35:14.432278Z","shell.execute_reply":"2023-09-13T18:35:14.73244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>4.2. <span style='color:#78D118'>|</span> Train and Validation</b>","metadata":{}},{"cell_type":"code","source":"# ---- ---- ---- #\n#      Train\n# ---- ---- ---- #\n\n# Dicom loader\n# ----\ntrain_dicom_loader = DICOMLoaderAugmentation(\n    train_df,\n    input_path          = TRAIN_DATASET_PATH,\n    scan_categories     = SCAN_CATEGORIES,\n    \n    # Image\n    num_imgs            = IMG_SEQ,\n    size                = IMG_SIZE,\n    rotate_angle        = IMG_ROTATE,\n    enable_center_focus = IMG_ENABLE_CENTRAL_FOCUS,\n    shuffle             = SHUFFLE,\n    \n    # Augmentation\n    fraction_augmented                = AUGMENTATION_FRACTION,\n    crop_limits                       = AUGMENTATION_CROP_LIMITS,\n    rotation_limits                   = AUGMENTATION_ROTATION_LIMITS,\n    translation_x_y_limits            = AUGMENTATION_TRANSLATION_X_Y_LIMITS,\n    blur_limits                       = AUGMENTATION_BLUR,\n    contrast_bright_alpha_beta_limits = AUGMENTATION_CONSTRAST_BRIGHT,\n    \n    # Settings\n    max_threads = MAX_THREADS_DICOM_LOADER,\n    seed        = SEED,\n    debug_mode =  DEBUG_DICOM_TRAIN_AUGMENTATION,\n)\n\n# Dataset\n# ----\ntrain_dataset = ScanDataset(\n    dicom_loader = train_dicom_loader,\n    batch_size   = BATCH_SIZE,\n    subset       = DatasetType.TRAIN.value,\n    shuffle      = SHUFFLE,\n    debug_mode   = DEBUG_SCANDATASET_TRAIN_AUGMENTATION\n)\n\n# ---- ---- ---- #\n#   Validation\n# ---- ---- ---- #\n\n# Dicom loader\n# ----\nval_dicom_loader = DICOMLoader(\n    valid_df,\n    input_path          = TRAIN_DATASET_PATH,\n    scan_categories     = SCAN_CATEGORIES,\n    num_imgs            = IMG_SEQ,\n    size                = IMG_SIZE,\n    scale               = IMG_SCALE,\n    rotate_angle        = IMG_ROTATE,\n    max_threads         = MAX_THREADS_DICOM_LOADER,\n    enable_center_focus = IMG_ENABLE_CENTRAL_FOCUS,\n    debug_mode          = DEBUG_DICOM_VALIDATION\n)\n\n# Dataset\n# ----\nval_dataset = ScanDataset(\n    dicom_loader = val_dicom_loader,\n    batch_size   = BATCH_SIZE,\n    subset       = DatasetType.VALIDATION.value,\n    shuffle      = False,\n    debug_mode   = DEBUG_SCANDATASET_VALIDATION\n)\n\n# ---- ---- ---- #\n#      Test\n# ---- ---- ---- #\n\n# Dicom loader\n# ----\ntest_dicom_loader = DICOMLoader(\n    test_df,\n    input_path          = TEST_DATASET_PATH,\n    scan_categories     = SCAN_CATEGORIES,\n    num_imgs            = IMG_SEQ,\n    size                = IMG_SIZE,\n    scale               = IMG_SCALE,\n    rotate_angle        = IMG_ROTATE,\n    max_threads         = MAX_THREADS_DICOM_LOADER,\n    enable_center_focus = IMG_ENABLE_CENTRAL_FOCUS,\n    debug_mode          = DEBUG_DICOM_TEST\n)\n\n# Dataset\n# ----\ntest_dataset = ScanDataset(\n    dicom_loader = test_dicom_loader,\n    batch_size   = BATCH_SIZE,\n    subset       = DatasetType.TEST.value,\n    shuffle      = False,\n    debug_mode   = DEBUG_SCANDATASET_TEST\n)","metadata":{"id":"-hVaUsKO76GM","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.735184Z","iopub.execute_input":"2023-09-13T18:35:14.735824Z","iopub.status.idle":"2023-09-13T18:35:14.748287Z","shell.execute_reply.started":"2023-09-13T18:35:14.735788Z","shell.execute_reply":"2023-09-13T18:35:14.747586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>4.3. <span style='color:#78D118'>|</span> Preview</b>","metadata":{}},{"cell_type":"code","source":"train_dicom_loader.show_all(0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.750911Z","iopub.execute_input":"2023-09-13T18:35:14.751914Z","iopub.status.idle":"2023-09-13T18:35:14.761578Z","shell.execute_reply.started":"2023-09-13T18:35:14.751889Z","shell.execute_reply":"2023-09-13T18:35:14.760665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.show_batch(0, columns=10, figure_size=(10, 10))","metadata":{"id":"Xds4E9Jh76GN","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.763502Z","iopub.execute_input":"2023-09-13T18:35:14.763936Z","iopub.status.idle":"2023-09-13T18:35:14.771395Z","shell.execute_reply.started":"2023-09-13T18:35:14.763903Z","shell.execute_reply":"2023-09-13T18:35:14.770404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>4.4 <span style='color:#78D118'>|</span> Summary</b>","metadata":{}},{"cell_type":"code","source":"def summary():\n    separator = \"\\n\" + \"---- \" * 20 + \"\\n\"\n    \n    def print_section(section_name, section_info):   \n        print(separator)\n        print(section_name)\n        print(\"\\n\")\n        for key, value in section_info:\n            print(f\"{key}: {value}\")\n        \n    version_section = [\n        (\"VERSION\", VERSION),\n    ]\n    print_section(\"Version\", version_section)\n    \n    # Image\n    image_section = [\n        (\"IMG_SEQ\", IMG_SEQ),\n        (\"IMG_SCALE\", IMG_SCALE),\n        (\"IMG_ROTATE\", IMG_ROTATE),\n        (\"IMG_ENABLE_CENTRAL_FOCUS\", IMG_ENABLE_CENTRAL_FOCUS),\n        (\"IMG_SHAPE\", INPUT_SHAPE),\n    ]\n    print_section(\"Image\", image_section)\n\n    # Model\n    model_section = [\n        (\"MODEL_NAME\", MODEL_NAME),\n        (\"BATCH_SIZE\", BATCH_SIZE),\n        (\"EPOCHS\", EPOCHS),\n        (\"OPTIMIZER\", type(COMPILE_OPTIMIZER)),\n        (\"COMPILE_LOSS\", COMPILE_LOSS),\n        (\"COMPILE_METRICS\", COMPILE_METRICS),\n        (\"SHUFFLE\", SHUFFLE),\n    ]\n    print_section(\"Model\", model_section)\n\n    # Train\n    # ----\n    print(separator)\n    print(\"Train\")\n    print(separator)\n    train_dicom_loader.summary()\n    print(separator)\n    train_dataset.summary()\n\n    # Val\n    # ----\n    print(separator)\n    print(\"Validation\")\n    print(separator)\n    val_dicom_loader.summary()\n    print(separator)\n    val_dataset.summary()\n    print(separator)\n    \n    \n    # Test\n    # ----\n    print(separator)\n    print(\"Test\")\n    print(separator)\n    test_dicom_loader.summary()\n    print(separator)\n    test_dataset.summary()\n    print(separator)\n\n\n\nsummary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T18:35:14.773169Z","iopub.execute_input":"2023-09-13T18:35:14.773675Z","iopub.status.idle":"2023-09-13T18:35:29.281294Z","shell.execute_reply.started":"2023-09-13T18:35:14.773642Z","shell.execute_reply":"2023-09-13T18:35:29.280308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:#78D118'>|</span> Model creation</b>","metadata":{}},{"cell_type":"code","source":"class DeepScanModel(Model):\n    def __init__(self, input_shape, model_name=\"My3DCNNModel\"):\n        # Define input layers\n        # ----\n        self.input_layers = [Input(shape=input_shape) for _ in range(4)]\n\n        # Build CNN models for each input\n        # ----\n        self.cnn_models = [self.build_cnn_branch(input_layer) for input_layer in self.input_layers]\n\n        # Concatenate outputs of CNN models\n        # ----\n        concatenated = concatenate(self.cnn_models)\n\n        # Add Global Average Pooling and Dense layers\n        # ----\n        x = self.build_head(concatenated)\n\n        # Define the final model\n        # ----\n        super(DeepScanModel, self).__init__(inputs=self.input_layers, outputs=x, name=model_name)\n\n    def build_cnn_branch(self, input_layer):        \n        x = Conv3D(64, 3)(input_layer)\n        x = ReLU()(x)\n        x = MaxPool3D(2)(x)\n        x = BatchNormalization()(x)\n        \n        x = Conv3D(128, 3)(x)\n        x = ReLU()(x)\n        x = MaxPool3D(2)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.1)(x)\n\n        x = Conv3D(256, 3)(x)\n        x = ReLU()(x)\n        x = MaxPool3D(2)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.2)(x)\n        \n        return x\n\n    def build_head(self, x):\n        x = GlobalAveragePooling3D()(x)\n\n        x = Dense(1024)(x)\n        x = ReLU()(x)\n        x = Dropout(0.3)(x)\n\n        x = Dense(1, activation=\"sigmoid\")(x)\n        \n        return x\n\n    def show_graph(self):\n        display(plot_model(self, show_shapes=True, show_layer_names=True))\n","metadata":{"id":"maH0FQOH-xXT","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:02:53.446809Z","iopub.execute_input":"2023-09-13T19:02:53.447175Z","iopub.status.idle":"2023-09-13T19:02:53.45951Z","shell.execute_reply.started":"2023-09-13T19:02:53.447146Z","shell.execute_reply":"2023-09-13T19:02:53.4583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DeepScanModel(input_shape=INPUT_SHAPE, model_name=MODEL_NAME)\nmodel.show_graph()","metadata":{"id":"0-KCs0dB76GN","outputId":"50e43b82-0dbe-4ef4-fe34-621341713bbe","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:02:55.156416Z","iopub.execute_input":"2023-09-13T19:02:55.157438Z","iopub.status.idle":"2023-09-13T19:02:56.031975Z","shell.execute_reply.started":"2023-09-13T19:02:55.157392Z","shell.execute_reply":"2023-09-13T19:02:56.031105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#78D118'>|</span> Model Training</b>","metadata":{}},{"cell_type":"code","source":"# Logs and models dir\n# ----\nmkdir(LOGS_PATH)\nmkdir(BEST_MODEL_PATH)\n\n# Each model\n# ----\nshow_text(\"sep\")\nshow_text(\"h3\",\"Run model\")\n\n# Callbacks bestmodel\n# ----\nbestmodel_callback = ModelCheckpoint(\n    filepath       = BEST_MODEL_H5_DIR,\n    verbose        = VERBOSITY,\n    monitor        = TF_CALL_BACK_BEST_MODEL_MONITOR,\n    mode           = 'max',\n    save_best_only = True\n)\n\n# Callbacks EarlyStopping\n# ----\nearlystopping_callback = EarlyStopping(\n    monitor   = TF_CALL_BACK_EARLY_STOP_MONITOR,\n    min_delta = 0,\n    patience  = TF_CALL_BACK_EARLY_STOP_PATIENTE,\n    verbose   = VERBOSITY,\n    mode      = 'auto',\n    baseline  = None,\n    restore_best_weights = True,\n)\n\n# Compile\n# ----\nmodel.compile(\n    optimizer = COMPILE_OPTIMIZER,\n    loss      = COMPILE_LOSS,\n    metrics   = COMPILE_METRICS \n)\n\n# Train\n# ----\nhistory = model.fit(\n    x               = train_dataset,\n    validation_data = val_dataset,\n    epochs          = EPOCHS,\n    shuffle         = SHUFFLE,\n    verbose         = VERBOSITY,\n    callbacks       = [bestmodel_callback, earlystopping_callback]\n)","metadata":{"id":"v3F7ebYY76GN","outputId":"8bbb77cd-7220-4516-ad6b-06da53a5fb70","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:02:57.882691Z","iopub.execute_input":"2023-09-13T19:02:57.883055Z","iopub.status.idle":"2023-09-13T19:13:29.114031Z","shell.execute_reply.started":"2023-09-13T19:02:57.883025Z","shell.execute_reply":"2023-09-13T19:13:29.111983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>7 <span style='color:#78D118'>|</span> Model Evaluation</b>","metadata":{}},{"cell_type":"code","source":"# Show result\n# ----\nshow_best_history(\n    history, \n    metric =\"auc\",\n    add_metric=[\"val_auc\"]\n)\n# Show history\n# ----\nshow_history(\n    history,\n    title = \"AUC History\",\n    y_label = \"AUC\",\n    metrics = [\"auc\", \"val_auc\"],\n    metric_labels = [\"Train AUC\", \"Validation AUC\"]\n)\n\nshow_history(\n    history,\n    title = \"Loss History\",\n    y_label = \"Loss\",\n    metrics = [\"loss\", \"val_loss\"],\n    metric_labels = [\"Train Loss\", \"Validation AUC\"]\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:13:29.11679Z","iopub.execute_input":"2023-09-13T19:13:29.117415Z","iopub.status.idle":"2023-09-13T19:13:29.743122Z","shell.execute_reply.started":"2023-09-13T19:13:29.117322Z","shell.execute_reply":"2023-09-13T19:13:29.74218Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>8 <span style='color:#78D118'>|</span> Submission</b>","metadata":{}},{"cell_type":"code","source":"model = load_model(BEST_MODEL_H5_DIR, custom_objects={'DeepScanModel': DeepScanModel})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:13:29.744621Z","iopub.execute_input":"2023-09-13T19:13:29.74497Z","iopub.status.idle":"2023-09-13T19:13:30.367996Z","shell.execute_reply.started":"2023-09-13T19:13:29.744936Z","shell.execute_reply":"2023-09-13T19:13:30.366939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(model, test_dataset, test_df):\n    predictions = []\n\n    for batch_idx in range(len(test_dataset)):\n        scan_type_1, scan_type_2, scan_type_3, scan_type_4 = test_dataset[batch_idx]\n        \n        batch_predictions = model.predict([scan_type_1, scan_type_2, scan_type_3, scan_type_4])\n\n        predictions.append(batch_predictions)\n\n    # Flatten the predictions list\n    # ----\n    submission = test_df.copy()\n    submission[\"Label\"] = [item[0] for sublist in predictions for item in sublist]\n    submission.rename(columns={\"ID\": \"BraTS21ID\", \"Label\": \"MGMT_value\"}, inplace=True)\n\n    return submission\n\nsubmission = generate_predictions(model, test_dataset, test_df)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_histogram(\n    submission[\"MGMT_value\"], \n    xlabel=\"MGMT value\", \n    ylabel=\"Qt\", \n    title=\"Result\"\n)\n\ndisplay(submission.head(100))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:30:23.126109Z","iopub.execute_input":"2023-09-13T19:30:23.126505Z","iopub.status.idle":"2023-09-13T19:30:23.489242Z","shell.execute_reply.started":"2023-09-13T19:30:23.126474Z","shell.execute_reply":"2023-09-13T19:30:23.488361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T19:29:31.520166Z","iopub.execute_input":"2023-09-13T19:29:31.520563Z","iopub.status.idle":"2023-09-13T19:29:31.530014Z","shell.execute_reply.started":"2023-09-13T19:29:31.52053Z","shell.execute_reply":"2023-09-13T19:29:31.529027Z"},"trusted":true},"execution_count":null,"outputs":[]}]}