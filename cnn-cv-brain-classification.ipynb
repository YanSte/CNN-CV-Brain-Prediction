{"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# | CNN | CV | RICE | Classification |\n## Convolutional Neural Networks (CNN) with Computer Vision (CV) for Brain Classification\n# <b>1 <span style='color:#78D118'>|</span> Introduction</b>\n\nGlioblastoma, the most common and aggressive form of brain cancer in adults, poses significant challenges in diagnosis and treatment. The presence of MGMT promoter methylation in the tumor has been identified as a crucial prognostic factor and an indicator of chemotherapy responsiveness. However, the current genetic analysis of brain cancer requires invasive procedures and time-consuming processes.\n\nThe objective of this project is to improve the diagnosis and treatment strategies for glioblastoma patients, minimizing the need for invasive procedures and streamlining the genetic analysis process. By leveraging radiogenomics, the aim is to develop a non-invasive method to predict the genetic profile of the tumor solely through imaging.\n\nTo address these issues, the Radiological Society of North America (RSNA) and the Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) have collaborated on a competition focusing on glioblastoma diagnosis and treatment planning. The competition involves using MRI scans to develop a model that can accurately predict the genetic subtype of glioblastoma by detecting the presence of MGMT promoter methylation.\n\nSuccessful outcomes from this competition will contribute to less invasive diagnostic procedures and more tailored treatment approaches for brain cancer patients. This abstract provides an overview of the project's objectives, the competition's context, and the potential impact on the management and survival rates of individuals affected by glioblastoma.\n\n## Research Efforts\nDuring the course of this project, we conducted extensive research to explore various methodologies for predicting the genetic subtype of glioblastoma based on MGMT promoter methylation. One of the methods we investigated was the use of the Unit-net architecture, a convolutional neural network designed specifically for medical image analysis.\n\n[| UNIT-NET | CV | BRAIN | Classification |](https://www.kaggle.com/code/yannicksteph/rsna-miccai-brain-tumor-classification)\n\nHowever, despite our efforts, we did not achieve conclusive results with the Unit-net model. The complexity and variability of glioblastoma tumors, as well as the limited availability of labeled data, presented significant challenges in training an effective Unit-net model for this task. As a result, we decided to pursue alternative approaches that showed more promise in accurately predicting the genetic subtype of glioblastoma.\n\n## Dataset Overview\n\nThe Radiological Society of North America (RSNAÂ®) is a non-profit organization representing 31 radiologic subspecialties from 145 countries worldwide. RSNA promotes excellence in patient care and healthcare delivery through education, research, and technological innovation.\n\nRSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world's largest radiology conference, and is dedicated to shaping the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its establishment. Additionally, RSNA actively supports and facilitates research in medical imaging artificial intelligence (AI) by sponsoring ongoing AI challenge competitions.\n\nThe Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) is committed to advancing research, education, and practice in the field of medical image computing, computer-assisted interventions, biomedical imaging, and medical robotics. The society achieves this objective by organizing high-quality international conferences, workshops, tutorials, and publications that promote the exchange and dissemination of advanced knowledge, expertise, and experiences produced by leading institutions, scientists, physicians, and educators worldwide.\n\nA complete list of acknowledgments can be found on this page.\n\n[RSNA-MICCAI Brain Tumor Radiogenomic Classification](https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification/data?select=train_labels.csv)\n\n## Objectives\n- Predict the genetic subtype of glioblastoma by detecting the presence of MGMT promoter methylation value between 0 to 1.\n\n## Implementation\nTo achieve the aforementioned objectives, we will follow these steps:\n\n- **Imports, Methods, Constants**\n- **Data Retrieval**\n- **Data Preparation** \n- **Model Creation** \n- **Model Training** \n- **Model Evaluation** \n\nBy following these steps, we aim to develop a CNN model capable predict the methylation value between 0 to 1.","metadata":{}},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Imports & Constants and Methods</b>\n\n## <b>2.1 <span style='color:#78D118'>|</span> Imports</b>","metadata":{}},{"cell_type":"code","source":"!pip install -q pydicom\n!pip install -q git+https://github.com/YanSteph/SKit.git","metadata":{"id":"oo5zZT0Z8dNL","outputId":"6bfa9cee-3e7a-4898-95ab-5471e105963b","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:35:09.067574Z","iopub.execute_input":"2023-09-09T11:35:09.068485Z","iopub.status.idle":"2023-09-09T11:36:08.959167Z","shell.execute_reply.started":"2023-09-09T11:35:09.068446Z","shell.execute_reply":"2023-09-09T11:36:08.957649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport numpy as np\nimport random\n\n# Dicom\nimport pydicom\n\n# Enum\nfrom enum import Enum\n\n# CV\nimport cv2\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import (\n    Model,\n    load_model\n)\nfrom tensorflow.keras.callbacks import (\n    Callback, \n    ModelCheckpoint, \n    EarlyStopping\n)\nfrom tensorflow.keras.layers import (\n    Input,\n    Conv3D,\n    BatchNormalization,\n    MaxPooling3D,\n    MaxPool3D,\n    Flatten,\n    Dense,\n    Dropout,\n    Resizing,\n    Rescaling,\n    RandomFlip,\n    RandomRotation,\n    concatenate,\n    GlobalAveragePooling3D,\n    Reshape,\n    LeakyReLU,\n    ReLU\n)\n\n# Keras\nimport keras\nfrom keras.utils.vis_utils import plot_model\n\n# Skit\nfrom skit.Debug import Debug\nfrom skit.InternalDebug import InternalDebug\nfrom skit.image import average_image_size\nfrom skit.dataset import stratifiedTrainValidSplit\nfrom skit.ModelMetrics import ModelMetrics\nfrom skit.Summarizable import Summarizable\nfrom skit.tensorflow import configure_gpu_memory\nfrom skit.dicom import (\n    DICOMLoader, \n    ImageFormat\n)\nfrom skit.utils import (\n    ls, \n    mkdir, \n    count_files\n)\nfrom skit.show import (\n    show_text, \n    show_images, \n    show_donut, \n    show_history, \n    show_confusion_matrix, \n    show_donut\n)","metadata":{"id":"v8J2Qtod76GK","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:08.962468Z","iopub.execute_input":"2023-09-09T11:36:08.962879Z","iopub.status.idle":"2023-09-09T11:36:19.440803Z","shell.execute_reply.started":"2023-09-09T11:36:08.962838Z","shell.execute_reply":"2023-09-09T11:36:19.439605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>2.3 <span style='color:#78D118'>|</span> Constants</b>","metadata":{}},{"cell_type":"code","source":"class MRIType(Enum):\n    FLAIR = \"FLAIR\"\n    T1w = \"T1w\"\n    T1wCE = \"T1wCE\"\n    T2w = \"T2w\"\n    \nclass DatasetType(Enum):\n    TRAIN = \"train\"\n    VALIDATION = \"validation\"\n    TEST = \"test\"","metadata":{"id":"7khZRVY576GL","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:19.442655Z","iopub.execute_input":"2023-09-09T11:36:19.443395Z","iopub.status.idle":"2023-09-09T11:36:19.450692Z","shell.execute_reply.started":"2023-09-09T11:36:19.443345Z","shell.execute_reply":"2023-09-09T11:36:19.449511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Global\n# ----\nVERSION         = \"V1\"\nVERBOSITY       = 2\nSEED            = 123\nSCAN_CATEGORIES = [mri_type.value for mri_type in MRIType]\nEXCLUDED_IDS    = [109, 123, 709]\n\n# Paths\n# ----\nRUN_DIR = './run'\nINPUT_PATH = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification\"\n\n# Train\nTRAIN_DATASET_PATH = INPUT_PATH + \"/train\"\nTRAIN_DATASET_DF_DIR = INPUT_PATH + \"/train_labels.csv\"\n\n# Test\nTEST_DATASET_PATH = INPUT_PATH + \"/test\"\nTEST_DATASET_DF_DIR = INPUT_PATH + \"/sample_submission.csv\"\n\n# Submission\nSUBMISSION_DATASET_DF_DIR = '/kaggle/working/submission.csv'\n\n# TF Callback Paths\n# ----\nLOGS_PATH = f'{RUN_DIR}/logs'\nBEST_MODEL_PATH = f'{RUN_DIR}/models'\nBEST_MODEL_H5_DIR = f'{BEST_MODEL_PATH}/model_{VERSION}.h5'\n\n# Fold\n# ----\nNUM_SPLIT_FOLDS = 5\nSELECTED_VALIDATION_FOLD = 1\n\n# Dicom Loader\n# ----\nMAX_THREADS_DICOM_LOADER = 8\n\n# Image\n# ----\nIMG_WIDTH_SIZE, IMG_HEIGHT_SIZE, IMG_CHAN = (128, 128, 1)\nIMG_SIZE = (IMG_WIDTH_SIZE, IMG_HEIGHT_SIZE)\n\nIMG_SEQ                  = 32\nIMG_SCALE                = .85\nIMG_ROTATE               = 0 \nIMG_ENABLE_CENTRAL_FOCUS = True\n\n# Augmentation\n# ----\nAUGMENTATION_FRACTION               = 0.5\nAUGMENTATION_CROP_LIMITS            = (0.85, 0.95)\nAUGMENTATION_ROTATION_LIMITS        = (4, 12)\nAUGMENTATION_TRANSLATION_X_Y_LIMITS = ((2, 6), (0, 2))\nAUGMENTATION_BLUR                   = (0.1, 0.15)\nAUGMENTATION_CONSTRAST_BRIGHT       = ((0.8, 1.2),(-2, 2))\n\n# Model\n# ----\nMODEL_NAME = \"Mult3DCNN4Input\"\nBATCH_SIZE = 8\nEPOCHS     = 30\nOPTIMIZER  = Adam(learning_rate=0.001)  \nSHUFFLE    = True\n\n# Format sample: (128, 128, 64, 1)\nINPUT_SHAPE    = (IMG_WIDTH_SIZE, IMG_HEIGHT_SIZE, IMG_SEQ, IMG_CHAN)\n\n# TF Callback\n# ----\nTF_CALL_BACK_BEST_MODEL_MONITOR  = 'accuracy'\nTF_CALL_BACK_EARLY_STOP_MONITOR  = 'accuracy'\nTF_CALL_BACK_EARLY_STOP_PATIENTE = 10","metadata":{"id":"PXWk1s0G76GL","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:19.454177Z","iopub.execute_input":"2023-09-09T11:36:19.454946Z","iopub.status.idle":"2023-09-09T11:36:22.785132Z","shell.execute_reply.started":"2023-09-09T11:36:19.454906Z","shell.execute_reply":"2023-09-09T11:36:22.784103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SEED is not None:\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:36:22.786366Z","iopub.execute_input":"2023-09-09T11:36:22.786757Z","iopub.status.idle":"2023-09-09T11:36:22.795463Z","shell.execute_reply.started":"2023-09-09T11:36:22.786722Z","shell.execute_reply":"2023-09-09T11:36:22.794433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Debug.set_debug_mode(False)\n\n# Train\nDEBUG_DICOM_TRAIN_AUGMENTATION       = False\nDEBUG_SCANDATASET_TRAIN_AUGMENTATION = False\n\n# Validation\nDEBUG_DICOM_VALIDATION       = False\nDEBUG_SCANDATASET_VALIDATION = False\n\n# Test\nDEBUG_DICOM_TEST       = False\nDEBUG_SCANDATASET_TEST = False","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:36:22.797707Z","iopub.execute_input":"2023-09-09T11:36:22.798638Z","iopub.status.idle":"2023-09-09T11:36:22.807827Z","shell.execute_reply.started":"2023-09-09T11:36:22.798599Z","shell.execute_reply":"2023-09-09T11:36:22.806516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>2.2 <span style='color:#78D118'>|</span> Methods</b>","metadata":{}},{"cell_type":"code","source":"class DICOMLoaderAugmentation(DICOMLoader):\n    def __init__(self,\n        df,\n        input_path,\n        scan_categories,\n        fraction_augmented                = 0,\n        crop_limits                       = None,\n        rotation_limits                   = None, \n        translation_x_y_limits            = None, \n        blur_limits                       = None,\n        contrast_bright_alpha_beta_limits = None,\n        num_imgs                          = None,\n        size                              = (224, 224),\n        scale                             = 1.0,\n        rotate_angle                      = 0,\n        enable_center_focus               = False,\n        id_column_name                    = \"ID\",\n        label_column_name                 = \"Label\",\n        image_format                      = ImageFormat.WHDC,\n        max_threads                       = 8,\n        image_file_sorter                 = lambda x: int(x[:-4].split(\"-\")[-1]),\n        shuffle                           = False,\n        seed                              = None,\n        debug_mode                        = False\n    ):        \n        self.__crop_limits                       = crop_limits\n        self.__rotation_limits                   = rotation_limits\n        self.__translation_x_y_limits            = translation_x_y_limits\n        self.__blur_limits                       = blur_limits\n        self.__contrast_bright_alpha_beta_limits = contrast_bright_alpha_beta_limits\n        self.__seed                              = seed\n        self.__debug                             = InternalDebug(debug_mode=debug_mode)\n        df = df.copy()\n        df = DICOMLoaderAugmentation.__data_augmentation(df, fraction_augmented, shuffle, self.__debug, seed)\n          \n        super().__init__(\n            df, \n            input_path,\n            scan_categories, \n            num_imgs, \n            size, \n            0, # NOTE: We define her the scale for augmented data\n            rotate_angle,\n            enable_center_focus,\n            id_column_name, \n            label_column_name, \n            image_format,\n            max_threads,\n            image_file_sorter,\n            False\n        )\n        \n    # ---------------- #\n    # Enum\n    # ---------------- #\n    \n    class OrigineType(Enum):\n        ORIGINAL = \"ORIGINAL\"\n        AUGMENTED = \"AUGMENTED\"\n\n    # ---------------- #\n    # Public\n    # ---------------- #\n\n    def load_all_scans(\n        self,\n        row,\n        show_progress=True\n    ):\n        self.__debug.log(\"== load_all_scans ==\")\n        \n        # Load all scans for the current index\n        # ----\n        scans_images = super().load_all_scans(row, show_progress)\n        \n        if self.__is_augmented(row):\n            scans_images = self.__augmented(row, scans_images)\n       \n        return scans_images\n    \n    # ---------------- #\n    # Private\n    # ---------------- #\n    \n    # ---- ---- ---- #\n    # Augmentation\n    # ---- ---- ---- #\n    \n    def __augmented(self, row, scans_images):\n        self.__debug.log(\"== augmented ==\")\n        \n        # Radom Crop\n        # ----\n        if self.__crop_limits is not None:\n            min_crop, max_crop = self.__crop_limits\n        \n            crop_random = random.uniform(min_crop, max_crop)\n        \n        # Radom Rotation\n        # ----\n        if self.__rotation_limits is not None:\n            min_rot, max_rot = self.__rotation_limits\n        \n            rotation_random = random.uniform(min_rot, max_rot) * random.choice([-1, 1])\n        \n        # Radom Translation \n        # ----\n        if self.__translation_x_y_limits is not None:\n            tx, ty = self.__translation_x_y_limits\n        \n            min_tx, max_tx = tx\n            min_ty, max_ty = ty\n        \n            tx_random = random.uniform(min_tx, max_tx) * random.choice([-1, 1])\n            ty_random = random.uniform(min_ty, max_ty) * random.choice([-1, 1])\n        \n        # Radom Blur \n        # ----\n        if self.__blur_limits is not None:\n            min_blur, max_blur = self.__blur_limits\n        \n            blur_random = random.uniform(min_blur, max_blur)\n\n        # Radom Contrast and Brightness\n        # ----\n        if self.__contrast_bright_alpha_beta_limits is not None:\n            alpha, meta = self.__contrast_bright_alpha_beta_limits\n        \n            min_alpha, max_alpha = alpha\n            min_beta, max_beta   = meta\n        \n            alpha_random = random.uniform(min_alpha, max_alpha)\n            beta_radom = random.uniform(min_beta, max_beta)\n        \n        # Log\n        # ----\n        self.__debug.log(\n                f\"Generation\\n\"\n                f\"- Crop: {crop_random}\\n\"\n                f\"- Rotation: {rotation_random}\\n\"\n                f\"- translation x: {tx_random}\\n\"\n                f\"- translation y: {ty_random}\\n\"\n                f\"- Blur: {blur_random}\\n\"\n                f\"- Contrast Brightness alpha: {alpha_random}\\n\"\n                f\"- Contrast Brightness Beta: {beta_radom}\\n\"               \n        )\n\n        for scan_type, images in scans_images.items():\n            \n            augmented_images = []\n            \n            # Format Normalize\n            # ----\n            images = self.format(images, \"normalize\")\n      \n            for image in images:\n                # Remove chan\n                # ----\n                image = np.squeeze(image, axis=-1)\n                \n                # Crop\n                # ----\n                if self.__crop_limits is not None:\n                    image = self.__crop_img(image, crop_random)\n                \n                # Resize\n                # ----\n                image = self._resize_img(image)\n                \n                # Rotation\n                # ----\n                if self.__rotation_limits is not None:\n                    image = self.__rotation_img(image, rotation_random)\n                \n                # Translation\n                # ----\n                if self.__translation_x_y_limits is not None:\n                    image = self.__translation_img(image, tx_random, ty_random)\n                \n                # Blur\n                # ----\n                if self.__blur_limits is not None:\n                    image = self.__blur_img(image, 3, blur_random)\n                \n                # Constrast and Brightness\n                # ----\n                if self.__contrast_bright_alpha_beta_limits is not None:\n                    image = self.__contrast_and_brightness_img(image, alpha_random, beta_radom)\n                  \n                # Save\n                # ----\n                augmented_images.append(image)\n        \n            # Numpy array\n            # ----\n            images = np.array(augmented_images)\n            \n            # Add chan\n            # ----\n            images = np.expand_dims(images, axis=-1)\n            \n            # Format Train\n            # ----\n            images = self.format(images, \"default\")\n\n            scans_images[scan_type] = images\n        \n        return scans_images\n\n    def __blur_img(self, image, kernel_size, blur_random):\n        return cv2.GaussianBlur(image, (kernel_size, kernel_size), blur_random)\n\n    def __translation_img(self, image, tx, ty):\n        height, width = image.shape[:2]\n        translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n        return cv2.warpAffine(image, translation_matrix, (width, height))\n    \n    def __rotation_img(self, image, rotation):\n        height, width = image.shape[:2]\n        center = (width / 2, height / 2)\n        rotation_matrix = cv2.getRotationMatrix2D(center, rotation, 1.0)\n        return cv2.warpAffine(image, rotation_matrix, (width, height))\n    \n    def __contrast_and_brightness_img(self,image, alpha, beta):\n        return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n    \n    def __crop_img(self, image, crop):\n        # Skip if no crop\n        # ----\n        if crop <= 0:\n            return image\n\n        # Calculate the center of the image\n        # ----\n        center_x, center_y = image.shape[1] / 2, image.shape[0] / 2\n\n        # Calculate the dimensions of the scaled image\n        # ----\n        width_scaled, height_scaled = image.shape[1] * crop, image.shape[0] * crop\n\n        # Calculate the coordinates for cropping the image\n        # ----\n        left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n        top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n\n        # Crop the image using the calculated coordinates\n        # ----\n        return image[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n    \n    def __is_augmented(self, row):\n        return self.df.loc[row, \"origine_type\"] == DICOMLoaderAugmentation.OrigineType.AUGMENTED.value\n    \n    # ---------------- #\n    # Private Static\n    # ---------------- #\n   \n    @staticmethod\n    def __data_augmentation(df, percentage, shuffle, debug, seed=None):\n        if percentage <= 0:\n            return df\n        debug.log(\"== __data_augmentation ==\")\n        \n        df[\"origine_type\"] = DICOMLoaderAugmentation.OrigineType.ORIGINAL.value\n        \n        # Calculate the number of times to copy\n        # ----\n        num_copies = int(percentage)\n        debug.log(\"num copies:\", num_copies)\n        rest_num_copies = DICOMLoaderAugmentation.__extract_first_decimal(percentage)\n        debug.log(\"rest copies:\", rest_num_copies, \"%\")\n        \n        # Calculate the number of rows to copy\n        # ----\n        num_rows = len(df)\n        debug.log(\"Initial size:\", num_rows)\n        \n        # Initialize a new DataFrame with copy the structure\n        # ----\n        augmented_df = df.copy().iloc[0:0]\n        \n        # Copy the DataFrame 'num_copies' times (525% = 500%)\n        # ----\n        for _ in range(num_copies):\n            random_sample = df.sample(n=num_rows, random_state=seed)\n            augmented_df = pd.concat([augmented_df, random_sample], ignore_index=True)\n        \n        # Copy the remaining percentage (525% = 25%)\n        # ----\n        if rest_num_copies > 0:\n            nombre_d_echantillons = int(num_rows * (rest_num_copies / 100))\n            echantillons_aleatoires = df.sample(n=nombre_d_echantillons, random_state=seed)\n            augmented_df = pd.concat([augmented_df, echantillons_aleatoires], ignore_index=True)\n               \n        # Identified as Augmented\n        # ----\n        augmented_df[\"origine_type\"] = DICOMLoaderAugmentation.OrigineType.AUGMENTED.value\n        debug.log(\"Augmented size add:\", len(augmented_df))\n        \n        # Merge with Original\n        # ----\n        df = pd.concat([df, augmented_df], ignore_index=True)\n        \n        # Suffle\n        # ----\n        if shuffle:\n            df = df.sample(frac=1, random_state=seed)\n          \n        # Reset Ids\n        # ----\n        df = df.reset_index(drop=True)\n        debug.log(\"Augmented size Total:\", len(df))\n        \n        return df\n    \n    @staticmethod\n    def __extract_first_decimal(number):\n        # Convert the number to a string for easier manipulation\n        number_str = str(number)\n\n        # Find the index of the decimal point\n        decimal_point_index = number_str.find(\".\")\n\n        # Extract the first decimal digit from the string\n        if decimal_point_index != -1 and decimal_point_index + 1 < len(number_str):\n            first_decimal_digit = number_str[decimal_point_index + 1]\n            \n            if len(first_decimal_digit) == 1:\n                first_decimal_digit += \"0\"\n        else:\n            # If there's no decimal part, return 0\n            first_decimal_digit = \"0\"\n\n        return int(first_decimal_digit)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:36:22.809772Z","iopub.execute_input":"2023-09-09T11:36:22.810497Z","iopub.status.idle":"2023-09-09T11:36:22.858393Z","shell.execute_reply.started":"2023-09-09T11:36:22.810458Z","shell.execute_reply":"2023-09-09T11:36:22.857187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ScanDataset(Sequence, Summarizable):\n    def __init__(\n        self,\n        dicom_loader,\n        batch_size,\n        subset     = \"train\",\n        shuffle    = True,\n        debug_mode = False\n    ):\n        \"\"\"\n        Initializes the ScanDataset object.\n\n        Parameters\n        ----------\n        dicom_loader : object\n            The DICOMLoader object to load DICOM images.\n        batch_size : int\n            The size of each batch.\n        subset: Subset of the data to return.\n          One of \"training\", \"validation\" or \"other\".\n          training and validation give the y_bath\n        shuffle : bool, optional\n            Whether to shuffle the dataset.\n        debug_mode : bool, optional\n            Whether to print debug information.\n        \"\"\"\n        self.__dicom_loader = dicom_loader\n        self.__batch_size   = batch_size\n        self.__is_trainable = subset.lower() in [\"validation\", \"train\"]\n        self.__shuffle      = shuffle\n        self.__debug        = InternalDebug(debug_mode=debug_mode)\n        self.__indices      = np.arange(self.__dicom_loader.len)\n        \n        if self.__shuffle:\n            np.random.shuffle(self.__indices)\n        \n    # ---------------- #\n    # Public methods\n    # ---------------- #\n\n    def show_batch(\n        self,\n        row,\n        columns=None,\n        figure_size=(5, 5),\n        color_map='hot'\n    ):\n        # Get the batch of images and labels (if in training mode)\n        # ----\n        if self.__is_trainable:\n            x_batch, y_batch = self[row]\n        else:\n            x_batch = self[row]\n            y_batch = None\n\n        # Determine the number of columns to show\n        # ----\n        if columns == None:\n            columns = self.__dicom_loader.num_imgs\n            \n        # Determine the number of input tensors\n        # ----\n        num_input_tensors = len(self.__dicom_loader.scan_categories)\n\n        # Loop through each sub-batch in the main batch\n        # ----\n        for i in range(len(x_batch[0])):\n            # Display the batch number and label (if available)\n            # ----\n            label_info = f\" Label: {y_batch[i]}\" if y_batch is not None else \"\"\n\n            # Loop through each input tensor\n            # ----\n            for j in range(num_input_tensors):\n                images = x_batch[j][i]\n                scan_type = self.__dicom_loader.scan_categories[j]\n\n                if self.__dicom_loader.image_format == ImageFormat.WHDC:\n                    images = ImageFormat.swap_dimensions(images, ImageFormat.DWHC)\n\n                # Generate labels for each image in the set\n                # ----\n                labels = [\n                            f\"Batch: {i + 1} \\nImg: {k + 1} \\nType: {scan_type} \\nLabel:{label_info}\"\n                            for k in range(len(images))\n                         ]\n\n                # Show\n                # ----\n                show_images(\n                    images,\n                    y=labels,\n                    columns=columns,\n                    figure_size=figure_size,\n                    color_map=color_map\n                )\n\n    # Overriding the summary method\n    def summary(self, train_dataset=None):\n        super().summary()\n\n        print(\"Additional summary details specific:\")\n\n        # 0 is the index of the first batch\n        # ----\n        if self.__is_trainable:\n            batch_x, batch_y = self[0]\n        else:\n            batch_x = self[0]\n\n        # Checking the batch format\n        # ----\n        print(\"Batch_x format:\")\n        for i, x in enumerate(batch_x):\n          print(f\"- Scan type {i+1}: {x.shape}\")\n\n        if self.__is_trainable:\n            print(f\"Batch_y format: {batch_y.shape}\")\n            \n        print(\"=\" * 40)\n\n    def on_epoch_end(self):\n        \"\"\"\n        Shuffles the dataset at the end of each epoch if shuffle is True.\n        \"\"\"\n        if self.__shuffle:\n            np.random.shuffle(self.__indices)\n\n    # ---------------- #\n    # Private methods\n    # ---------------- #\n\n    def __getitem__(self, ids):\n        self.__debug.log(\"== __getitem__ ==\")\n        \"\"\"\n        Retrieves a batch of data by batch index.\n\n        Parameters\n        ----------\n        ids : int\n            The batch index.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the batch of images and labels.\n        \"\"\"\n        # Calculate the start and end indices for the batch\n        # ----\n        from_id = ids * self.__batch_size\n        to_id = (ids + 1) * self.__batch_size\n\n        self.__debug.log(f\"Batch ID: {ids}\")\n         \n        # Get the indices and labels for the current batch\n        # ----\n        batch_indices = self.__indices[from_id: to_id]\n   \n        self.__debug.log(\"batch_indices:\", batch_indices.tolist())\n\n        batches_y = []\n\n        # Initialize a list to hold batches for each input tensor\n        # ----\n        batches_x = [[] for _ in range(len(self.__dicom_loader.scan_categories))]\n\n        # Loop through each index in the batch\n        # ----\n        for i in batch_indices:\n            self.__debug.log(\"Processing batch index:\", i)\n\n            # Store label\n            # ----\n            label = self.__dicom_loader.gel_label(i)\n            batches_y.append(label)\n            self.__debug.log(\"Label:\", label)\n\n            # Load all scans for the current index\n            # ----\n            batch_x_image_paths = self.__dicom_loader.load_all_scans(i, show_progress=False)\n\n            # Loop through each scan type and its corresponding images\n            # ----\n            for j, (scan_type, images) in enumerate(batch_x_image_paths.items()):\n                self.__debug.log(\"Processing Scan Type:\", scan_type, \"Number of Images Loaded:\", len(images))\n                batches_x[j].append(images)\n\n        # Convert to batch x y\n        # ----\n        batch_x = [np.array(b) for b in batches_x]\n        batch_y = np.array(batches_y)\n\n        self.__debug.log(f\"Final batch shapes - batch_x: {[x.shape for x in batch_x]}, batch_y: {batch_y}\")\n\n        # Return the image batches and labels if in training mode, otherwise just the image batches\n        # ----\n        if self.__is_trainable:\n            return batch_x, batch_y\n        else:\n            return batch_x\n\n    def __len__(self):\n        \"\"\"\n        Calculates the number of batches in the dataset.\n\n        Returns\n        -------\n        int\n            The number of batches.\n        \"\"\"\n        return int(np.ceil(self.__dicom_loader.len / self.__batch_size))","metadata":{"id":"3EV7Q3Jk76GM","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:22.860593Z","iopub.execute_input":"2023-09-09T11:36:22.861012Z","iopub.status.idle":"2023-09-09T11:36:22.889181Z","shell.execute_reply.started":"2023-09-09T11:36:22.860972Z","shell.execute_reply":"2023-09-09T11:36:22.888002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>3. <span style='color:#78D118'>|</span> Data Retrieval</b>\n\n","metadata":{}},{"cell_type":"code","source":"# Train\ntrain_df = pd.read_csv(TRAIN_DATASET_DF_DIR)\ntrain_df.rename(columns = { \"BraTS21ID\": \"ID\", \"MGMT_value\": \"Label\"}, inplace=True)\n\nindex_to_remove = train_df[train_df['ID'].isin(EXCLUDED_IDS)].index\ntrain_df.drop(index_to_remove, inplace=True)\n\ntrain_df.reset_index(drop=True, inplace=True)\nshow_text(\"b\", \"Train\", False)\ndisplay(train_df.head())\n\n# Test\n\ntest_df = pd.read_csv(TEST_DATASET_DF_DIR)\ntest_df.rename(columns = { \"BraTS21ID\": \"ID\", \"MGMT_value\": \"Label\"}, inplace=True)\nshow_text(\"b\", \"Test\", False)\ndisplay(test_df.head())","metadata":{"id":"GA2m949u76GL","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:22.891174Z","iopub.execute_input":"2023-09-09T11:36:22.892043Z","iopub.status.idle":"2023-09-09T11:36:22.972502Z","shell.execute_reply.started":"2023-09-09T11:36:22.892002Z","shell.execute_reply":"2023-09-09T11:36:22.971435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b>4. <span style='color:#78D118'>|</span> Data Preparation</b>","metadata":{}},{"cell_type":"markdown","source":"### <b>4.1. <span style='color:#78D118'>|</span> Split Data</b>","metadata":{}},{"cell_type":"code","source":"# Split Train, valid using Stratified K-Folds\n# ----\ntrain_df, valid_df = stratifiedTrainValidSplit(\n    train_df,\n    x_feature_columns = ['ID'],\n    y_target_columns = ['Label'],\n    num_splits = NUM_SPLIT_FOLDS,\n    selected_fold = SELECTED_VALIDATION_FOLD,\n    seed = SEED,\n    shuffle = SHUFFLE\n)\n\n# Show\n# ----\nshow_donut(\n    [len(train_df), len(valid_df)],\n    [\"Train\", \"Validation\"], # TODO: Add test\n    colors = [\"lightsteelblue\",\"coral\"],\n    figsize = (8,8),\n    title = \"Dataset Distribution\"\n)","metadata":{"id":"4q3Ii1hY76GM","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:22.980909Z","iopub.execute_input":"2023-09-09T11:36:22.983758Z","iopub.status.idle":"2023-09-09T11:36:23.471842Z","shell.execute_reply.started":"2023-09-09T11:36:22.983712Z","shell.execute_reply":"2023-09-09T11:36:23.470796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>4.2. <span style='color:#78D118'>|</span> Train and Validation</b>","metadata":{}},{"cell_type":"code","source":"# ---- ---- ---- #\n#      Train\n# ---- ---- ---- #\n\n# Dicom loader\n# ----\ntrain_dicom_loader = DICOMLoaderAugmentation(\n    train_df,\n    input_path          = TRAIN_DATASET_PATH,\n    scan_categories     = SCAN_CATEGORIES,\n    \n    # Image\n    num_imgs            = IMG_SEQ,\n    size                = IMG_SIZE,\n    scale               = IMG_SCALE,\n    rotate_angle        = IMG_ROTATE,\n    enable_center_focus = IMG_ENABLE_CENTRAL_FOCUS,\n    shuffle             = SHUFFLE,\n    \n    # Augmentation\n    fraction_augmented                = AUGMENTATION_FRACTION,\n    crop_limits                       = AUGMENTATION_CROP_LIMITS,\n    rotation_limits                   = AUGMENTATION_ROTATION_LIMITS,\n    translation_x_y_limits            = AUGMENTATION_TRANSLATION_X_Y_LIMITS,\n    blur_limits                       = AUGMENTATION_BLUR,\n    contrast_bright_alpha_beta_limits = AUGMENTATION_CONSTRAST_BRIGHT,\n    \n    # Settings\n    max_threads = MAX_THREADS_DICOM_LOADER,\n    seed        = SEED,\n    debug_mode =  DEBUG_DICOM_TRAIN_AUGMENTATION,\n)\n\n# Dataset\n# ----\ntrain_dataset = ScanDataset(\n    dicom_loader = train_dicom_loader,\n    batch_size   = BATCH_SIZE,\n    subset       = DatasetType.TRAIN.value,\n    shuffle      = SHUFFLE,\n    debug_mode   = DEBUG_SCANDATASET_TRAIN_AUGMENTATION\n)\n\n# ---- ---- ---- #\n#   Validation\n# ---- ---- ---- #\n\n# Dicom loader\n# ----\nval_dicom_loader = DICOMLoader(\n    valid_df,\n    input_path          = TRAIN_DATASET_PATH,\n    scan_categories     = SCAN_CATEGORIES,\n    num_imgs            = IMG_SEQ,\n    size                = IMG_SIZE,\n    scale               = IMG_SCALE,\n    rotate_angle        = IMG_ROTATE,\n    max_threads         = MAX_THREADS_DICOM_LOADER,\n    enable_center_focus = IMG_ENABLE_CENTRAL_FOCUS,\n    debug_mode          = DEBUG_DICOM_VALIDATION\n)\n\n# Dataset\n# ----\nval_dataset = ScanDataset(\n    dicom_loader = val_dicom_loader,\n    batch_size   = BATCH_SIZE,\n    subset       = DatasetType.VALIDATION.value,\n    shuffle      = False,\n    debug_mode   = DEBUG_SCANDATASET_VALIDATION\n)\n\n# ---- ---- ---- #\n#      Test\n# ---- ---- ---- #\n\n# Dicom loader\n# ----\ntest_dicom_loader = DICOMLoader(\n    test_df,\n    input_path          = TEST_DATASET_PATH,\n    scan_categories     = SCAN_CATEGORIES,\n    num_imgs            = IMG_SEQ,\n    size                = IMG_SIZE,\n    scale               = IMG_SCALE,\n    rotate_angle        = IMG_ROTATE,\n    max_threads         = MAX_THREADS_DICOM_LOADER,\n    enable_center_focus = IMG_ENABLE_CENTRAL_FOCUS,\n    debug_mode          = DEBUG_DICOM_TEST\n)\n\n# Dataset\n# ----\ntest_dataset = ScanDataset(\n    dicom_loader = test_dicom_loader,\n    batch_size   = BATCH_SIZE,\n    subset       = DatasetType.TEST.value,\n    shuffle      = False,\n    debug_mode   = DEBUG_SCANDATASET_TEST\n)","metadata":{"id":"-hVaUsKO76GM","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:23.476907Z","iopub.execute_input":"2023-09-09T11:36:23.479683Z","iopub.status.idle":"2023-09-09T11:36:23.505432Z","shell.execute_reply.started":"2023-09-09T11:36:23.479624Z","shell.execute_reply":"2023-09-09T11:36:23.504290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>4.3. <span style='color:#78D118'>|</span> Preview</b>","metadata":{}},{"cell_type":"code","source":"#train_dicom_loader.show_all(0)\n#print(\"\\n\\n\\n\\n\\n\\n\")\n#val_dicom_loader.show_all(0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:23.511182Z","iopub.execute_input":"2023-09-09T11:36:23.513877Z","iopub.status.idle":"2023-09-09T11:36:23.525178Z","shell.execute_reply.started":"2023-09-09T11:36:23.513835Z","shell.execute_reply":"2023-09-09T11:36:23.519632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_dataset.show_batch(0, columns=10, figure_size=(10, 10))\n#print(\"\\n\\n\\n\\n\\n\\n\")\n#val_dataset.show_batch(10, columns=10, figure_size=(10, 10))","metadata":{"id":"Xds4E9Jh76GN","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:23.528119Z","iopub.execute_input":"2023-09-09T11:36:23.529120Z","iopub.status.idle":"2023-09-09T11:36:23.534671Z","shell.execute_reply.started":"2023-09-09T11:36:23.529075Z","shell.execute_reply":"2023-09-09T11:36:23.533642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summary():\n    separator = \"\\n\" + \"---- \" * 20 + \"\\n\"\n    \n    def print_section(section_name, section_info):   \n        print(separator)\n        print(section_name)\n        print(\"\\n\")\n        for key, value in section_info:\n            print(f\"{key}: {value}\")\n        \n    version_section = [\n        (\"VERSION\", VERSION),\n    ]\n    print_section(\"Version\", version_section)\n    \n    # Image\n    image_section = [\n        (\"IMG_SEQ\", IMG_SEQ),\n        (\"IMG_SCALE\", IMG_SCALE),\n        (\"IMG_ROTATE\", IMG_ROTATE),\n        (\"IMG_ENABLE_CENTRAL_FOCUS\", IMG_ENABLE_CENTRAL_FOCUS),\n        (\"IMG_SHAPE\", INPUT_SHAPE),\n    ]\n    print_section(\"Image\", image_section)\n\n    def get_optimizer():\n        if isinstance(OPTIMIZER, Adam):\n            return (\"LEARNING_RATE\", OPTIMIZER.get_config()[\"learning_rate\"])\n        else:\n            return (\"OPTIMIZER\", OPTIMIZER)\n        \n    # Model\n    model_section = [\n        (\"MODEL_NAME\", MODEL_NAME),\n        (\"BATCH_SIZE\", BATCH_SIZE),\n        (\"EPOCHS\", EPOCHS),\n        get_optimizer(),\n        (\"SHUFFLE\", SHUFFLE),\n    ]\n    print_section(\"Model\", model_section)\n\n    # Train\n    # ----\n    print(separator)\n    train_dicom_loader.summary()\n    print(separator)\n    train_dataset.summary()\n\n    # Val\n    # ----\n    print(separator)\n    val_dicom_loader.summary()\n    print(separator)\n    val_dataset.summary()\n    print(separator)\n    \n    \n    # Test\n    # ----\n    print(separator)\n    test_dicom_loader.summary()\n    print(separator)\n    test_dataset.summary()\n    print(separator)\n\n\n\nsummary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-09T11:36:23.536819Z","iopub.execute_input":"2023-09-09T11:36:23.537680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:#78D118'>|</span> Model creation</b>","metadata":{}},{"cell_type":"code","source":"class DeepScanModel(Model):\n    def __init__(self, input_shape, model_name=\"My3DCNNModel\"):\n        super(DeepScanModel, self).__init__()\n\n        # Define input layers\n        # ----\n        self.input_layers = [Input(shape=input_shape) for _ in range(4)]\n\n        # Build CNN models for each input\n        # ----\n        self.cnn_models = [self.build_cnn_branch(input_layer) for input_layer in self.input_layers]\n\n        # Concatenate outputs of CNN models\n        # ----\n        concatenated = concatenate(self.cnn_models)\n\n        # Add Global Average Pooling and Dense layers\n        # ----\n        x = self.build_head(concatenated)\n\n        # Define the final model\n        # ----\n        super(DeepScanModel, self).__init__(inputs=self.input_layers, outputs=x, name=model_name)\n\n    def build_cnn_branch(self, input_layer):        \n        x = Conv3D(64, 3)(input_layer)\n        x = ReLU()(x)\n        x = MaxPool3D(2)(x)\n        x = BatchNormalization()(x)\n        \n        x = Conv3D(128, 3)(x)\n        x = ReLU()(x)\n        x = MaxPool3D(2)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.1)(x)\n\n        x = Conv3D(256, 3)(x)\n        x = ReLU()(x)\n        x = MaxPool3D(2)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.2)(x)\n        \n        return x\n\n    def build_head(self, x):\n        x = GlobalAveragePooling3D()(x)\n\n        x = Dense(1024)(x)\n        x = ReLU()(x)\n        x = Dropout(0.3)(x)\n\n        x = Dense(1, activation=\"sigmoid\")(x)\n        \n        return x\n\n    def show_graph(self):\n        display(plot_model(self, show_shapes=True, show_layer_names=True))","metadata":{"id":"maH0FQOH-xXT","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DeepScanModel(input_shape=INPUT_SHAPE, model_name=MODEL_NAME)\nmodel.show_graph()","metadata":{"id":"0-KCs0dB76GN","outputId":"50e43b82-0dbe-4ef4-fe34-621341713bbe","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:#78D118'>|</span> Model Training</b>","metadata":{}},{"cell_type":"code","source":"# Logs and models dir\n# ----\nmkdir(LOGS_PATH)\nmkdir(BEST_MODEL_PATH)\n\n# ModelPerformanceReport\n# ----\nreport = ModelMetrics([VERSION])\n\n# Each model\n# ----\nshow_text(\"sep\")\nshow_text(\"h3\",\"Run model\")\n\n# Callbacks bestmodel\n# ----\nbestmodel_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath       = BEST_MODEL_H5_DIR,\n    verbose        = VERBOSITY,\n    monitor        = TF_CALL_BACK_BEST_MODEL_MONITOR,\n    save_best_only = True\n)\nreport.add_best_model_path(VERSION, BEST_MODEL_H5_DIR)\n\n# Callbacks EarlyStopping\n# ----\nearlystopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor   = TF_CALL_BACK_EARLY_STOP_MONITOR,\n    min_delta = 0,\n    patience  = TF_CALL_BACK_EARLY_STOP_PATIENTE,\n    verbose   = VERBOSITY,\n    mode      = 'auto',\n    baseline  = None,\n    restore_best_weights = True\n)\n\n# Time Start\n# ----\nreport.start_timer(VERSION)\n\n# Compile\n# ----\n\nmodel.compile(\n    optimizer = OPTIMIZER,\n    loss      = 'binary_crossentropy',\n    metrics   = ['accuracy']\n)\n\n# Train\n# ----\nhistory = model.fit(\n    x               = train_dataset,\n    validation_data = val_dataset,\n    epochs          = EPOCHS,\n    shuffle         = SHUFFLE,\n    verbose         = VERBOSITY,\n    callbacks       = [earlystopping_callback, bestmodel_callback]#, clr_callback]\n)\n\n# Time End\n# ----\nreport.stop_timer(VERSION)\n\n# Save history\n# ----\nreport.add_history(VERSION, history)","metadata":{"id":"v3F7ebYY76GN","outputId":"8bbb77cd-7220-4516-ad6b-06da53a5fb70","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#78D118'>|</span> Model Evaluation</b>","metadata":{}},{"cell_type":"code","source":"# Show result\n# ----\nreport.show_best_result(VERSION)\n\n# Show history\n# ----\nreport.show_history(VERSION)\nreport.show_report()\n\nbest_model_path = report.get_best_model_path()\n\nmodel = load_model(best_model_path)\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Iterate Through the Dataset and Make Predictions\n#predictions = []\n#sample_ids = []  # To store the IDs associated with each prediction\n\n#for batch_idx in range(len(test_dataset)):\n#    batch_data = test_dataset[batch_idx]\n    \n    # Extract the batch of images from the dataset\n#    images = batch_data[0]  # Assuming images are in the first element of the batch\n#    batch_ids = batch_data[1]  # Assuming IDs are in the second element of the batch\n#    print(batch_ids)\n    # Make predictions using your model\n   # batch_predictions = model.predict(images)\n    \n    # Append the batch predictions and IDs to the lists\n#    predictions.append(batch_predictions)\n#    sample_ids.extend(batch_ids)  # Extend the IDs list\n\n# Step 4: Convert Predictions and IDs to a Pandas DataFrame\n# Flatten the predictions and IDs lists\n#flat_predictions = [item for sublist in predictions for item in sublist]\n#flat_sample_ids = [item for sublist in sample_ids for item in sublist]\n\n# Create a Pandas DataFrame\n#results_df = pd.DataFrame({'Sample_ID': flat_sample_ids, 'Predictions': flat_predictions})\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#78D118'>|</span> Submission</b>","metadata":{}},{"cell_type":"code","source":"\n#merged_df.to_csv(SUBMISSION_DATASET_DF_DIR, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}